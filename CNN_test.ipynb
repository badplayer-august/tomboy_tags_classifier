{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb38b101-2096-455f-8996-53811c2d8e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 10:11:38.293654: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-13 10:11:38.293686: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pathlib\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as po\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import CategoricalNB\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6292b4a7-4bea-4e3c-b960-2335a1013f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex = {'girl': '1girl', 'tomboy': '1girl', 'boy': '1boy', 'trap': '1boy'}\n",
    "# labels = ['girl', 'tomboy', 'boy', 'trap']\n",
    "\n",
    "# df_raw = dict()\n",
    "# feature_raw = dict()\n",
    "# image_raw = dict()\n",
    "\n",
    "# for label in labels:\n",
    "#     df_raw[label] = pd.read_csv('./images/' + label + '.csv', header=0, converters={'tags':lambda x: x.split()}).sample(4500)\n",
    "#     df_raw[label]['Rating'] = df_raw[label]['tags'].apply(lambda x: x[-4][7:])\n",
    "#     df_raw[label]['tags'] = df_raw[label]['tags'].apply(lambda x: x[x.index(sex[label]):-4])\n",
    "#     df_raw[label]['tags'] = df_raw[label]['tags'].apply(lambda x: [e for e in x if e != sex[label] and e != label])\n",
    "#     # df_raw[label]['flag'] = df_raw[label]['tags'].apply(lambda x: ('female_focus' in x) or ('male_focus' in x))\n",
    "#     counter = Counter()\n",
    "#     for row in df_raw[label]['tags']:\n",
    "#         counter.update(row)\n",
    "#     feature_raw[label] = np.array([[k, v] for k, v in zip(counter.keys(), counter.values())], dtype=object)\n",
    "#     feature_raw[label] = feature_raw[label][np.lexsort((feature_raw[label][:, 0], feature_raw[label][:, 1]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d55e0d4-9b72-40b4-ac41-c9d53908a1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# for label in labels:\n",
    "#     for ID in df_raw[label]['id'].values:\n",
    "#         img = np.array(tf.keras.utils.load_img('./images/' + label + '/' + str(ID) + '.jpg', color_mode='grayscale'))\n",
    "#         X.append(img)\n",
    "#         y.append(label)\n",
    "\n",
    "# X = np.array(X)\n",
    "# X = X.reshape(4500*4, 128, 128, 1)\n",
    "# y = np.array(y)\n",
    "# indices = np.arange(X.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "# X, y = X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c53b54c4-05cd-453b-8e2e-fa7a7fad18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNN:\n",
    "#     def __init__(self, labels):\n",
    "#         self.label_encoder = tf.keras.layers.StringLookup(\n",
    "#             output_mode='one_hot',\n",
    "#             vocabulary=labels,\n",
    "#         )\n",
    "#         self.test_model = tf.keras.Sequential([\n",
    "#             tf.keras.layers.Rescaling(1./255),\n",
    "#             tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(128,128)),\n",
    "#             tf.keras.layers.MaxPooling2D(),\n",
    "#             tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#             tf.keras.layers.MaxPooling2D(),\n",
    "#             tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "#             tf.keras.layers.MaxPooling2D(),\n",
    "#             tf.keras.layers.Flatten(),\n",
    "#             tf.keras.layers.Dense(128, activation='relu'),\n",
    "#             tf.keras.layers.Dense(self.label_encoder.vocabulary_size())\n",
    "#         ])\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.model = tf.keras.models.clone_model(self.test_model)\n",
    "#         self.model.compile(\n",
    "#             loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"]\n",
    "#         )\n",
    "#         self.model.fit(X, self.label_encoder(y).numpy(), batch_size=128, epochs=10, validation_split=0.2)\n",
    "#     def predict(self, X):\n",
    "#         y = self.model.predict(X)\n",
    "#         return np.array([self.label_encoder.get_vocabulary()[x] for x in np.argmax(y, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d0b2ab-7156-4ce1-b8fe-1a6ae6c47262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN()\n",
    "# model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c472e50e-2c3f-42dd-89fc-96653d285236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trans_bg(arr):\n",
    "#     temp = arr.copy()\n",
    "#     temp[arr == 'tomboy'] = 'girl'\n",
    "#     temp[arr == 'girl'] = 'girl'\n",
    "#     temp[arr == 'trap'] = 'boy'\n",
    "#     temp[arr == 'boy'] = 'boy'\n",
    "#     return temp\n",
    "# y_bg = trans_bg(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc6be2a-9746-4205-b81e-a46ed6db87b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN(['girl', 'boy'])\n",
    "# model.fit(X, y_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa656596-bc38-40ce-b26d-187124e327d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def trans_bgish(arr):\n",
    "#     temp = arr.copy()\n",
    "#     temp[arr == 'tomboy'] = 'girlish'\n",
    "#     temp[arr == 'girl'] = 'boyish'\n",
    "#     temp[arr == 'trap'] = 'girlish'\n",
    "#     temp[arr == 'boy'] = 'boyish'\n",
    "#     return temp\n",
    "# y_bgish = trans_bgish(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b8a6073-313a-47de-ba7c-debe3b64babb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN(['girlish', 'boyish'])\n",
    "# model.fit(X, y_bgish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e79e9b52-f5b3-441f-aa60-58cc1e69a7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 files belonging to 2 classes.\n",
      "Using 14400 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 10:11:40.923511: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-01-13 10:11:40.923535: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-01-13 10:11:40.923552: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archlinux): /proc/driver/nvidia/version does not exist\n",
      "2022-01-13 10:11:40.923766: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 files belonging to 2 classes.\n",
      "Using 3600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "data_dir = './Dataset_bg'\n",
    "batch_size = 32\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    color_mode='grayscale',\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    color_mode='grayscale',\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a172a4-d3f5-41cb-9104-b9503fe6728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c97fb56-4af3-4347-a021-e0650beba1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = tf.keras.Sequential([ \n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "982c2acb-96fc-4464-9c58-4e0c2167901a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "450/450 [==============================] - 129s 285ms/step - loss: 0.6906 - accuracy: 0.5304 - val_loss: 0.6871 - val_accuracy: 0.5367\n",
      "Epoch 2/10\n",
      "450/450 [==============================] - 130s 289ms/step - loss: 0.6753 - accuracy: 0.5755 - val_loss: 0.6807 - val_accuracy: 0.5683\n",
      "Epoch 3/10\n",
      "450/450 [==============================] - 128s 284ms/step - loss: 0.6633 - accuracy: 0.5980 - val_loss: 0.6931 - val_accuracy: 0.5628\n",
      "Epoch 4/10\n",
      "450/450 [==============================] - 128s 284ms/step - loss: 0.6449 - accuracy: 0.6238 - val_loss: 0.7036 - val_accuracy: 0.5564\n",
      "Epoch 5/10\n",
      "450/450 [==============================] - 128s 284ms/step - loss: 0.5969 - accuracy: 0.6728 - val_loss: 0.7480 - val_accuracy: 0.5481\n",
      "Epoch 6/10\n",
      "450/450 [==============================] - 128s 284ms/step - loss: 0.5173 - accuracy: 0.7314 - val_loss: 0.8252 - val_accuracy: 0.5489\n",
      "Epoch 7/10\n",
      "450/450 [==============================] - 127s 283ms/step - loss: 0.4370 - accuracy: 0.7835 - val_loss: 1.0702 - val_accuracy: 0.5319\n",
      "Epoch 8/10\n",
      "450/450 [==============================] - 127s 282ms/step - loss: 0.3688 - accuracy: 0.8267 - val_loss: 1.2078 - val_accuracy: 0.5419\n",
      "Epoch 9/10\n",
      "450/450 [==============================] - 127s 283ms/step - loss: 0.2972 - accuracy: 0.8631 - val_loss: 1.3759 - val_accuracy: 0.5356\n",
      "Epoch 10/10\n",
      "450/450 [==============================] - 127s 283ms/step - loss: 0.2411 - accuracy: 0.8933 - val_loss: 1.5844 - val_accuracy: 0.5378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9da45a2280>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59f80df9-46e2-46d8-85b0-705492ae80e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18000 files belonging to 2 classes.\n",
      "Using 14400 files for training.\n",
      "Found 18000 files belonging to 2 classes.\n",
      "Using 3600 files for validation.\n"
     ]
    }
   ],
   "source": [
    "data_dir = './Dataset_bgish'\n",
    "batch_size = 32\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    color_mode='grayscale',\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    color_mode='grayscale',\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41bcc152-87cd-4084-835d-a1146d24a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dffff42f-c47a-4e34-ae48-cc7689634008",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "962240da-a0ca-4fd0-a872-6f5820f11955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "450/450 [==============================] - 131s 291ms/step - loss: 0.6798 - accuracy: 0.5664 - val_loss: 0.6639 - val_accuracy: 0.6081\n",
      "Epoch 2/10\n",
      "450/450 [==============================] - 130s 288ms/step - loss: 0.6634 - accuracy: 0.6062 - val_loss: 0.6550 - val_accuracy: 0.6289\n",
      "Epoch 3/10\n",
      "450/450 [==============================] - 130s 289ms/step - loss: 0.6526 - accuracy: 0.6196 - val_loss: 0.6546 - val_accuracy: 0.6344\n",
      "Epoch 4/10\n",
      "450/450 [==============================] - 130s 288ms/step - loss: 0.6384 - accuracy: 0.6381 - val_loss: 0.6661 - val_accuracy: 0.6186\n",
      "Epoch 5/10\n",
      "450/450 [==============================] - 131s 292ms/step - loss: 0.6154 - accuracy: 0.6639 - val_loss: 0.7161 - val_accuracy: 0.6061\n",
      "Epoch 6/10\n",
      "450/450 [==============================] - 131s 291ms/step - loss: 0.5677 - accuracy: 0.7037 - val_loss: 0.7642 - val_accuracy: 0.6053\n",
      "Epoch 7/10\n",
      "450/450 [==============================] - 128s 283ms/step - loss: 0.4953 - accuracy: 0.7522 - val_loss: 0.8601 - val_accuracy: 0.6008\n",
      "Epoch 8/10\n",
      "450/450 [==============================] - 141s 313ms/step - loss: 0.4098 - accuracy: 0.8037 - val_loss: 0.9952 - val_accuracy: 0.5883\n",
      "Epoch 9/10\n",
      "450/450 [==============================] - 167s 370ms/step - loss: 0.3220 - accuracy: 0.8553 - val_loss: 1.1768 - val_accuracy: 0.5778\n",
      "Epoch 10/10\n",
      "450/450 [==============================] - 149s 332ms/step - loss: 0.2712 - accuracy: 0.8819 - val_loss: 1.3584 - val_accuracy: 0.5714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9da4545a00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52328577-a627-44e2-b9af-19c41907e3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy/girl:\n",
      " \n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 128s 284ms/step - loss: 0.6633 - accuracy: 0.5980 - val_loss: 0.6931 - val_accuracy: 0.5628\n",
      " \n",
      "boyish\\girlish:\n",
      " \n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 130s 289ms/step - loss: 0.6526 - accuracy: 0.6196 - val_loss: 0.6546 - val_accuracy: 0.6344\n",
      "\n"
     ]
    }
   ],
   "source": [
    "str1 = '''\n",
    "Epoch 3/3\n",
    "450/450 [==============================] - 128s 284ms/step - loss: 0.6633 - accuracy: 0.5980 - val_loss: 0.6931 - val_accuracy: 0.5628\n",
    "'''\n",
    "str2 = '''\n",
    "Epoch 3/3\n",
    "450/450 [==============================] - 130s 289ms/step - loss: 0.6526 - accuracy: 0.6196 - val_loss: 0.6546 - val_accuracy: 0.6344\n",
    "'''\n",
    "\n",
    "print('boy/girl:\\n', str1, '\\nboyish\\girlish:\\n', str2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
